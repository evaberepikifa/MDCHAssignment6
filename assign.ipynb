{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2      3      4    5    6      7    8    9    10   11   12  13\n",
      "0  63.0  1.0  1.0  145.0  233.0  1.0  2.0  150.0  0.0  2.3  3.0  0.0  6.0   0\n",
      "1  67.0  1.0  4.0  160.0  286.0  0.0  2.0  108.0  1.0  1.5  2.0  3.0  3.0   2\n",
      "2  67.0  1.0  4.0  120.0  229.0  0.0  2.0  129.0  1.0  2.6  2.0  2.0  7.0   1\n",
      "3  37.0  1.0  3.0  130.0  250.0  0.0  0.0  187.0  0.0  3.5  3.0  0.0  3.0   0\n",
      "4  41.0  0.0  2.0  130.0  204.0  0.0  2.0  172.0  0.0  1.4  1.0  0.0  3.0   0\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv('processed.cleveland.data', header=None)\n",
    "data.columns\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define target and variable \n",
    "X = data.drop(13, axis=1)\n",
    "y = data[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      3      4      7    9    11\n",
      "0    63.0  145.0  233.0  150.0  2.3  0.0\n",
      "1    67.0  160.0  286.0  108.0  1.5  3.0\n",
      "2    67.0  120.0  229.0  129.0  2.6  2.0\n",
      "3    37.0  130.0  250.0  187.0  3.5  0.0\n",
      "4    41.0  130.0  204.0  172.0  1.4  0.0\n",
      "..    ...    ...    ...    ...  ...  ...\n",
      "298  45.0  110.0  264.0  132.0  1.2  0.0\n",
      "299  68.0  144.0  193.0  141.0  3.4  2.0\n",
      "300  57.0  130.0  131.0  115.0  1.2  1.0\n",
      "301  57.0  130.0  236.0  174.0  0.0  1.0\n",
      "302  38.0  138.0  175.0  173.0  0.0  NaN\n",
      "\n",
      "[303 rows x 6 columns]\n",
      "      1    2    5    6    8    10   12\n",
      "0    1.0  1.0  1.0  2.0  0.0  3.0  6.0\n",
      "1    1.0  4.0  0.0  2.0  1.0  2.0  3.0\n",
      "2    1.0  4.0  0.0  2.0  1.0  2.0  7.0\n",
      "3    1.0  3.0  0.0  0.0  0.0  3.0  3.0\n",
      "4    0.0  2.0  0.0  2.0  0.0  1.0  3.0\n",
      "..   ...  ...  ...  ...  ...  ...  ...\n",
      "298  1.0  1.0  0.0  0.0  0.0  2.0  7.0\n",
      "299  1.0  4.0  1.0  0.0  0.0  2.0  7.0\n",
      "300  1.0  4.0  0.0  0.0  1.0  2.0  7.0\n",
      "301  0.0  2.0  0.0  2.0  0.0  2.0  3.0\n",
      "302  1.0  3.0  0.0  0.0  0.0  1.0  3.0\n",
      "\n",
      "[303 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "continuous_cols= X.iloc[:, [0, 3, 4, 7, 9, 11]]\n",
    "categorical_cols = X.iloc[:, [1, 2, 5, 6, 8, 10, 12]]\n",
    "print(continuous_cols)\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputers : mean and mode\n",
    "imp_cat = SimpleImputer(strategy='most_frequent')\n",
    "imp_cont = SimpleImputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing values\n",
    "X_cont_imputed = pd.DataFrame(imp_cont.fit_transform(continuous_cols), columns=continuous_cols.columns)\n",
    "X_cat_imputed = pd.DataFrame(imp_cat.fit_transform(categorical_cols), columns=categorical_cols.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1    2    5    6    8    10   12\n",
      "0    1.0  1.0  1.0  2.0  0.0  3.0  6.0\n",
      "1    1.0  4.0  0.0  2.0  1.0  2.0  3.0\n",
      "2    1.0  4.0  0.0  2.0  1.0  2.0  7.0\n",
      "3    1.0  3.0  0.0  0.0  0.0  3.0  3.0\n",
      "4    0.0  2.0  0.0  2.0  0.0  1.0  3.0\n",
      "..   ...  ...  ...  ...  ...  ...  ...\n",
      "298  1.0  1.0  0.0  0.0  0.0  2.0  7.0\n",
      "299  1.0  4.0  1.0  0.0  0.0  2.0  7.0\n",
      "300  1.0  4.0  0.0  0.0  1.0  2.0  7.0\n",
      "301  0.0  2.0  0.0  2.0  0.0  2.0  3.0\n",
      "302  1.0  3.0  0.0  0.0  0.0  1.0  3.0\n",
      "\n",
      "[303 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_cat_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#one hot encode categorical variable    \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "ohe_cat = encoder.fit_transform(X_cat_imputed).toarray()\n",
    "print(ohe_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1_1.0  2_2.0  2_3.0  2_4.0  5_1.0  6_1.0  6_2.0  8_1.0  10_2.0  10_3.0  12_6.0  12_7.0\n",
      "0      1.0    0.0    0.0    0.0    1.0    0.0    1.0    0.0     0.0     1.0     1.0     0.0\n",
      "1      1.0    0.0    0.0    1.0    0.0    0.0    1.0    1.0     1.0     0.0     0.0     0.0\n",
      "2      1.0    0.0    0.0    1.0    0.0    0.0    1.0    1.0     1.0     0.0     0.0     1.0\n",
      "3      1.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0     0.0     1.0     0.0     0.0\n",
      "4      0.0    1.0    0.0    0.0    0.0    0.0    1.0    0.0     0.0     0.0     0.0     0.0\n",
      "..     ...    ...    ...    ...    ...    ...    ...    ...     ...     ...     ...     ...\n",
      "298    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     1.0     0.0     0.0     1.0\n",
      "299    1.0    0.0    0.0    1.0    1.0    0.0    0.0    0.0     1.0     0.0     0.0     1.0\n",
      "300    1.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0     1.0     0.0     0.0     1.0\n",
      "301    0.0    1.0    0.0    0.0    0.0    0.0    1.0    0.0     1.0     0.0     0.0     0.0\n",
      "302    1.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0\n",
      "\n",
      "[303 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_cat_imputed.columns = X_cat_imputed.columns.astype(str)\n",
    "# One-hot encode categorical variable\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "ohe_cat = encoder.fit_transform(X_cat_imputed)\n",
    "ohe_cat = pd.DataFrame(ohe_cat, columns=encoder.get_feature_names_out(X_cat_imputed.columns))\n",
    "print(ohe_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.94872647  0.75752504 -0.2649003   0.01719733  1.08733806 -0.72309499]\n",
      " [ 1.39200191  1.61121989  0.76041519 -1.82190531  0.39718162  2.50385129]\n",
      " [ 1.39200191 -0.6652997  -0.34228261 -0.90235399  1.34614673  1.42820253]\n",
      " ...\n",
      " [ 0.28381332 -0.0961698  -2.23814899 -1.51538821  0.13837295  0.35255377]\n",
      " [ 0.28381332 -0.0961698  -0.20686358  1.06811312 -0.89686172  0.35255377]\n",
      " [-1.82174501  0.35913411 -1.38694368  1.02432497 -0.89686172  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#scale continuous variable\n",
    "scaler = StandardScaler()\n",
    "X_cont_scaled = scaler.fit_transform(X_cont_imputed)\n",
    "print(X_cont_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#one hot encode target variable\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_encoded = to_categorical(y,num_classes=5)\n",
    "print(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.         ...  0.01719733  1.08733806\n",
      "  -0.72309499]\n",
      " [ 1.          0.          0.         ... -1.82190531  0.39718162\n",
      "   2.50385129]\n",
      " [ 1.          0.          0.         ... -0.90235399  1.34614673\n",
      "   1.42820253]\n",
      " ...\n",
      " [ 1.          0.          0.         ... -1.51538821  0.13837295\n",
      "   0.35255377]\n",
      " [ 0.          1.          0.         ...  1.06811312 -0.89686172\n",
      "   0.35255377]\n",
      " [ 1.          0.          1.         ...  1.02432497 -0.89686172\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#combine into one X\n",
    "X_combined = np.concatenate([ohe_cat, X_cont_scaled ], axis=1)\n",
    "print(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5 neurons in the hidden layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eva.berepiki\\AppData\\Local\\miniconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation accuracy with 5 neurons: 0.3721\n",
      "\n",
      " 10 neurons in the hidden layer\n",
      "Final validation accuracy with 10 neurons: 0.5814\n",
      "\n",
      " 15 neurons in the hidden layer\n",
      "Final validation accuracy with 15 neurons: 0.6047\n",
      "\n",
      " 20 neurons in the hidden layer\n",
      "Final validation accuracy with 20 neurons: 0.5349\n",
      "\n",
      " 25 neurons in the hidden layer\n",
      "Final validation accuracy with 25 neurons: 0.6279\n",
      "\n",
      " 30 neurons in the hidden layer\n",
      "Final validation accuracy with 30 neurons: 0.5581\n",
      "\n",
      " 35 neurons in the hidden layer\n",
      "Final validation accuracy with 35 neurons: 0.6047\n",
      "\n",
      " 40 neurons in the hidden layer\n",
      "Final validation accuracy with 40 neurons: 0.6279\n",
      "\n",
      " 45 neurons in the hidden layer\n",
      "Final validation accuracy with 45 neurons: 0.5814\n",
      "\n",
      " 50 neurons in the hidden layer\n",
      "Final validation accuracy with 50 neurons: 0.6047\n",
      "\n",
      "Best number of neurons: 25 with validation accuracy: 0.6279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_neurons = 0\n",
    "best_val_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "# Loop through different hidden layer sizes\n",
    "for neurons in range(5, 55, 5):\n",
    "    print(f\" {neurons} neurons in the hidden layer\")\n",
    "    \n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        Dense(neurons, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(5, activation='softmax')  \n",
    "    ])\n",
    "    \n",
    "    # Compile  model\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=25,\n",
    "        validation_split=0.2,\n",
    "        verbose=0 \n",
    "    )\n",
    "    \n",
    "    # Print final validation accuracy\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    print(f\"Final validation accuracy with {neurons} neurons: {val_accuracy:.4f}\\n\")\n",
    "\n",
    "    # Find the best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_neurons = neurons\n",
    "        best_model = model\n",
    "\n",
    "print(f\"Best number of neurons: {best_neurons} with validation accuracy: {best_val_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1837\n",
      "Test Accuracy: 0.5385\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test data\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
