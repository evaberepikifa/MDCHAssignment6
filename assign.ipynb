{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2      3      4    5    6      7    8    9    10   11   12  13\n",
      "0  63.0  1.0  1.0  145.0  233.0  1.0  2.0  150.0  0.0  2.3  3.0  0.0  6.0   0\n",
      "1  67.0  1.0  4.0  160.0  286.0  0.0  2.0  108.0  1.0  1.5  2.0  3.0  3.0   2\n",
      "2  67.0  1.0  4.0  120.0  229.0  0.0  2.0  129.0  1.0  2.6  2.0  2.0  7.0   1\n",
      "3  37.0  1.0  3.0  130.0  250.0  0.0  0.0  187.0  0.0  3.5  3.0  0.0  3.0   0\n",
      "4  41.0  0.0  2.0  130.0  204.0  0.0  2.0  172.0  0.0  1.4  1.0  0.0  3.0   0\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv('processed.cleveland.data', header=None)\n",
    "data.columns\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define target and variable \n",
    "X = data.drop(13, axis=1)\n",
    "y = data[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      3      4      7    9    11\n",
      "0    63.0  145.0  233.0  150.0  2.3  0.0\n",
      "1    67.0  160.0  286.0  108.0  1.5  3.0\n",
      "2    67.0  120.0  229.0  129.0  2.6  2.0\n",
      "3    37.0  130.0  250.0  187.0  3.5  0.0\n",
      "4    41.0  130.0  204.0  172.0  1.4  0.0\n",
      "..    ...    ...    ...    ...  ...  ...\n",
      "298  45.0  110.0  264.0  132.0  1.2  0.0\n",
      "299  68.0  144.0  193.0  141.0  3.4  2.0\n",
      "300  57.0  130.0  131.0  115.0  1.2  1.0\n",
      "301  57.0  130.0  236.0  174.0  0.0  1.0\n",
      "302  38.0  138.0  175.0  173.0  0.0    ?\n",
      "\n",
      "[303 rows x 6 columns]\n",
      "      1    2    5    6    8    10   12\n",
      "0    1.0  1.0  1.0  2.0  0.0  3.0  6.0\n",
      "1    1.0  4.0  0.0  2.0  1.0  2.0  3.0\n",
      "2    1.0  4.0  0.0  2.0  1.0  2.0  7.0\n",
      "3    1.0  3.0  0.0  0.0  0.0  3.0  3.0\n",
      "4    0.0  2.0  0.0  2.0  0.0  1.0  3.0\n",
      "..   ...  ...  ...  ...  ...  ...  ...\n",
      "298  1.0  1.0  0.0  0.0  0.0  2.0  7.0\n",
      "299  1.0  4.0  1.0  0.0  0.0  2.0  7.0\n",
      "300  1.0  4.0  0.0  0.0  1.0  2.0  7.0\n",
      "301  0.0  2.0  0.0  2.0  0.0  2.0  3.0\n",
      "302  1.0  3.0  0.0  0.0  0.0  1.0  3.0\n",
      "\n",
      "[303 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "continuous_cols= X.iloc[:, [0, 3, 4, 7, 9, 11]]\n",
    "categorical_cols = X.iloc[:, [1, 2, 5, 6, 8, 10, 12]]\n",
    "print(continuous_cols)\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputers : mean and mode\n",
    "imp_cat = SimpleImputer(strategy='most_frequent')\n",
    "imp_cont = SimpleImputer(strategy='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use mean strategy with non-numeric data:\ncould not convert string to float: '?'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#impute missing values\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_cont_imputed = pd.DataFrame(\u001b[43mimp_cont\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontinuous_cols\u001b[49m\u001b[43m)\u001b[49m, columns=continuous_cols.columns)\n\u001b[32m      3\u001b[39m X_cat_imputed = pd.DataFrame(imp_cat.fit_transform(categorical_cols), columns=categorical_cols.columns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eva.berepiki\\AppData\\Local\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eva.berepiki\\AppData\\Local\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:918\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    903\u001b[39m         warnings.warn(\n\u001b[32m    904\u001b[39m             (\n\u001b[32m    905\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    914\u001b[39m         )\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    920\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eva.berepiki\\AppData\\Local\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eva.berepiki\\AppData\\Local\\miniconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:434\u001b[39m, in \u001b[36mSimpleImputer.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    418\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the imputer on `X`.\u001b[39;00m\n\u001b[32m    419\u001b[39m \n\u001b[32m    420\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    432\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    436\u001b[39m     \u001b[38;5;66;03m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[39;00m\n\u001b[32m    437\u001b[39m     \u001b[38;5;66;03m# otherwise\u001b[39;00m\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eva.berepiki\\AppData\\Local\\miniconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:361\u001b[39m, in \u001b[36mSimpleImputer._validate_input\u001b[39m\u001b[34m(self, X, in_fit)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcould not convert\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n\u001b[32m    356\u001b[39m     new_ve = \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    357\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot use \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m strategy with non-numeric data:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    358\u001b[39m             \u001b[38;5;28mself\u001b[39m.strategy, ve\n\u001b[32m    359\u001b[39m         )\n\u001b[32m    360\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ve\n",
      "\u001b[31mValueError\u001b[39m: Cannot use mean strategy with non-numeric data:\ncould not convert string to float: '?'"
     ]
    }
   ],
   "source": [
    "#impute missing values\n",
    "X_cont_imputed = pd.DataFrame(imp_cont.fit_transform(continuous_cols), columns=continuous_cols.columns)\n",
    "X_cat_imputed = pd.DataFrame(imp_cat.fit_transform(categorical_cols), columns=categorical_cols.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1    2    5    6    8    10   12\n",
      "0    1.0  1.0  1.0  2.0  0.0  3.0  6.0\n",
      "1    1.0  4.0  0.0  2.0  1.0  2.0  3.0\n",
      "2    1.0  4.0  0.0  2.0  1.0  2.0  7.0\n",
      "3    1.0  3.0  0.0  0.0  0.0  3.0  3.0\n",
      "4    0.0  2.0  0.0  2.0  0.0  1.0  3.0\n",
      "..   ...  ...  ...  ...  ...  ...  ...\n",
      "298  1.0  1.0  0.0  0.0  0.0  2.0  7.0\n",
      "299  1.0  4.0  1.0  0.0  0.0  2.0  7.0\n",
      "300  1.0  4.0  0.0  0.0  1.0  2.0  7.0\n",
      "301  0.0  2.0  0.0  2.0  0.0  2.0  3.0\n",
      "302  1.0  3.0  0.0  0.0  0.0  1.0  3.0\n",
      "\n",
      "[303 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_cat_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 1. 1. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#one hot encode categorical variable    \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first')\n",
    "ohe_cat = encoder.fit_transform(X_cat_imputed).toarray()\n",
    "print(ohe_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1_1.0  2_2.0  2_3.0  2_4.0  5_1.0  6_1.0  6_2.0  8_1.0  10_2.0  10_3.0  12_6.0  12_7.0\n",
      "0      1.0    0.0    0.0    0.0    1.0    0.0    1.0    0.0     0.0     1.0     1.0     0.0\n",
      "1      1.0    0.0    0.0    1.0    0.0    0.0    1.0    1.0     1.0     0.0     0.0     0.0\n",
      "2      1.0    0.0    0.0    1.0    0.0    0.0    1.0    1.0     1.0     0.0     0.0     1.0\n",
      "3      1.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0     0.0     1.0     0.0     0.0\n",
      "4      0.0    1.0    0.0    0.0    0.0    0.0    1.0    0.0     0.0     0.0     0.0     0.0\n",
      "..     ...    ...    ...    ...    ...    ...    ...    ...     ...     ...     ...     ...\n",
      "298    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     1.0     0.0     0.0     1.0\n",
      "299    1.0    0.0    0.0    1.0    1.0    0.0    0.0    0.0     1.0     0.0     0.0     1.0\n",
      "300    1.0    0.0    0.0    1.0    0.0    0.0    0.0    1.0     1.0     0.0     0.0     1.0\n",
      "301    0.0    1.0    0.0    0.0    0.0    0.0    1.0    0.0     1.0     0.0     0.0     0.0\n",
      "302    1.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0     0.0     0.0     0.0     0.0\n",
      "\n",
      "[303 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_cat_imputed.columns = X_cat_imputed.columns.astype(str)\n",
    "# One-hot encode categorical variable\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "ohe_cat = encoder.fit_transform(X_cat_imputed)\n",
    "ohe_cat = pd.DataFrame(ohe_cat, columns=encoder.get_feature_names_out(X_cat_imputed.columns))\n",
    "print(ohe_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.94872647  0.75752504 -0.2649003   0.01719733  1.08733806 -0.72309499]\n",
      " [ 1.39200191  1.61121989  0.76041519 -1.82190531  0.39718162  2.50385129]\n",
      " [ 1.39200191 -0.6652997  -0.34228261 -0.90235399  1.34614673  1.42820253]\n",
      " ...\n",
      " [ 0.28381332 -0.0961698  -2.23814899 -1.51538821  0.13837295  0.35255377]\n",
      " [ 0.28381332 -0.0961698  -0.20686358  1.06811312 -0.89686172  0.35255377]\n",
      " [-1.82174501  0.35913411 -1.38694368  1.02432497 -0.89686172  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#scale continuous variable\n",
    "scaler = StandardScaler()\n",
    "X_cont_scaled = scaler.fit_transform(X_cont_imputed)\n",
    "print(X_cont_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#one hot encode target variable\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_encoded = to_categorical(y,num_classes=5)\n",
    "print(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.          0.         ...  0.01719733  1.08733806\n",
      "  -0.72309499]\n",
      " [ 1.          0.          0.         ... -1.82190531  0.39718162\n",
      "   2.50385129]\n",
      " [ 1.          0.          0.         ... -0.90235399  1.34614673\n",
      "   1.42820253]\n",
      " ...\n",
      " [ 1.          0.          0.         ... -1.51538821  0.13837295\n",
      "   0.35255377]\n",
      " [ 0.          1.          0.         ...  1.06811312 -0.89686172\n",
      "   0.35255377]\n",
      " [ 1.          0.          1.         ...  1.02432497 -0.89686172\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#combine into one X\n",
    "X_combined = np.concatenate([ohe_cat, X_cont_scaled ], axis=1)\n",
    "print(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 5 neurons in the hidden layer...\n",
      "Final validation accuracy with 5 neurons: 0.5581\n",
      "\n",
      "Training with 10 neurons in the hidden layer...\n",
      "Final validation accuracy with 10 neurons: 0.5581\n",
      "\n",
      "Training with 15 neurons in the hidden layer...\n",
      "Final validation accuracy with 15 neurons: 0.5349\n",
      "\n",
      "Training with 20 neurons in the hidden layer...\n",
      "Final validation accuracy with 20 neurons: 0.6744\n",
      "\n",
      "Training with 25 neurons in the hidden layer...\n",
      "Final validation accuracy with 25 neurons: 0.6047\n",
      "\n",
      "Training with 30 neurons in the hidden layer...\n",
      "Final validation accuracy with 30 neurons: 0.5814\n",
      "\n",
      "Training with 35 neurons in the hidden layer...\n",
      "Final validation accuracy with 35 neurons: 0.5581\n",
      "\n",
      "Training with 40 neurons in the hidden layer...\n",
      "Final validation accuracy with 40 neurons: 0.6279\n",
      "\n",
      "Training with 45 neurons in the hidden layer...\n",
      "Final validation accuracy with 45 neurons: 0.6512\n",
      "\n",
      "Training with 50 neurons in the hidden layer...\n",
      "Final validation accuracy with 50 neurons: 0.6047\n",
      "\n",
      "Best number of neurons: 20 with validation accuracy: 0.6744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_neurons = 0\n",
    "best_val_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "# Loop through different hidden layer sizes\n",
    "for neurons in range(5, 55, 5):\n",
    "    print(f\" {neurons} neurons in the hidden layer\")\n",
    "    \n",
    "    # Define the model\n",
    "    model = Sequential([\n",
    "        Dense(neurons, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(5, activation='softmax')  \n",
    "    ])\n",
    "    \n",
    "    # Compile  model\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=25,\n",
    "        validation_split=0.2,\n",
    "        verbose=0 \n",
    "    )\n",
    "    \n",
    "    # Print final validation accuracy\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    print(f\"Final validation accuracy with {neurons} neurons: {val_accuracy:.4f}\\n\")\n",
    "\n",
    "    # Find the best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_neurons = neurons\n",
    "        best_model = model\n",
    "\n",
    "print(f\"Best number of neurons: {best_neurons} with validation accuracy: {best_val_accuracy:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1291\n",
      "Test Accuracy: 0.5714\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best model on the test data\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
